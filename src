Code :

import pandas as pd
import numpy as np
import re
import nltk
import tensorflow as tf
from tensorflow import keras
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tensorflow.keras import mixed_precision

# Enable Mixed Precision for Faster Computation
mixed_precision.set_global_policy('mixed_float16')

# Set GPU Memory Growth to Avoid Allocation Issues
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)

# Download NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

# Load the CSV file
df = pd.read_csv("sentiment140/training.1600000.processed.noemoticon.csv", encoding="latin-1", header=None)

# Rename columns
colnames = ['sentiment', 'id', 'date', 'query', 'user', 'text']
df.columns = colnames

# Replace sentiment values (4 to 1 for binary classification)
df['sentiment'].replace({4: 1}, inplace=True)

df.head()

# Initialize stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
pattern = re.compile(r'[^a-zA-Z]')

# Text cleaning function
def clean_text(text):
    text = pattern.sub(' ', text)  # Remove non-alphabet characters
    text = text.lower().split()
    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]
    return ' '.join(text)

# Apply text cleaning
df['processed_text'] = df['text'].apply(clean_text)

# Reduce dataset size for experimentation
df_sampled = df.sample(frac=0.1, random_state=42)  # Use only 10% of data
X = df_sampled['processed_text']
y = df_sampled['sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=10)

# Convert text to numerical data using TF-IDF
vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))
X_train = vectorizer.fit_transform(X_train).toarray()
X_test = vectorizer.transform(X_test).toarray()

X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)
X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)
y_train = tf.convert_to_tensor(y_train.values, dtype=tf.float32)
y_test = tf.convert_to_tensor(y_test.values, dtype=tf.float32)

nn_model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, input_shape=(X_train.shape[1],), activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')  # Use sigmoid for binary classification
])

nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

nn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

prediction = nn_model.predict(X_test)

history = nn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Model evaluation
loss, accuracy = nn_model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

predictions = nn_model.predict(X_test)
predictions = (predictions > 0.5).astype(int)  # Convert probabilities to binary output

print("Sample Predictions:")
print(predictions[:5].flatten())

import pandas as pd
import numpy as np
import re
import nltk
import tensorflow as tf
from tensorflow import keras
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tensorflow.keras import mixed_precision

# Enable Mixed Precision for Faster Computation
mixed_precision.set_global_policy('mixed_float16')

# Set GPU Memory Growth to Avoid Allocation Issues
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)

# Download NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

# Load the CSV file
df = pd.read_csv("sentiment140/training.1600000.processed.noemoticon.csv", encoding="latin-1", header=None)

# Rename columns
colnames = ['sentiment', 'id', 'date', 'query', 'user', 'text']
df.columns = colnames

# Replace sentiment values (4 to 1 for binary classification)
df['sentiment'].replace({4: 1}, inplace=True)

df.head()

# Initialize stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
pattern = re.compile(r'[^a-zA-Z]')

# Text cleaning function
def clean_text(text):
    text = pattern.sub(' ', text)  # Remove non-alphabet characters
    text = text.lower().split()
    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]
    return ' '.join(text)

# Apply text cleaning
df['processed_text'] = df['text'].apply(clean_text)

# Reduce dataset size for experimentation
df_sampled = df.sample(frac=0.1, random_state=42)  # Use only 10% of data
X = df_sampled['processed_text']
y = df_sampled['sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=10)

# Convert text to numerical data using TF-IDF
vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))
X_train = vectorizer.fit_transform(X_train).toarray()
X_test = vectorizer.transform(X_test).toarray()

X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)
X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)
y_train = tf.convert_to_tensor(y_train.values, dtype=tf.float32)
y_test = tf.convert_to_tensor(y_test.values, dtype=tf.float32)

nn_model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, input_shape=(X_train.shape[1],), activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')  # Use sigmoid for binary classification
])

nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

nn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

prediction = nn_model.predict(X_test)

history = nn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Model evaluation
loss, accuracy = nn_model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

predictions = nn_model.predict(X_test)
predictions = (predictions > 0.5).astype(int)  # Convert probabilities to binary output

print("Sample Predictions:")
print(predictions[:5].flatten())

import pickle
import tensorflow as tf
from tensorflow.keras.models import load_model
import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import nltk

# Setup
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
pattern = re.compile(r'[^a-zA-Z]')

# Load saved model and vectorizer
model = load_model('sentiment_model.h5')
with open('tfidf_vectorizer.pkl', 'rb') as f:
    vectorizer = pickle.load(f)

# Text cleaning function
def clean_text(text):
    text = pattern.sub(' ', text)
    text = text.lower().split()
    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]
    return ' '.join(text)

# Get new input
new_text = input("Enter your mood in text: ")
cleaned = clean_text(new_text)

# Transform input
vectorized = vectorizer.transform([cleaned]).toarray()
vectorized_tensor = tf.convert_to_tensor(vectorized, dtype=tf.float32)

# Predict
result = model.predict(vectorized_tensor)
label = "Positive" if result[0][0] >= 0.5 else "Negative"
print(f"Sentiment: {label}")
